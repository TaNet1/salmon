---
title: 数据清洗｜为什么数据清洗非常重要？
date: 2024-09-11 14:11:05
tags: 
	- "PM"
	- "Product"
	- "Note"
	- "Learn"
categories: AI产品经理
thumbnail: "/images/ai6.jpg"
banner: "/images/ai6.jpg"
excerpt: "干净、准确的数据对于训练 ML 模型尤其重要。"
---

### 什么是数据清洗？
数据清洗是为机器学习（ML）和商业智能（BI）应用程序准备原始数据的一个必不可少的过程。原始数据可能会包含许多错误，这可能会影响 ML 模型的准确性，并导致预测不正确和出现负面的业务影响。 

数据清洗的关键步骤包括修改和删除不正确和不完整的数据字段、识别和删除重复信息和不相关数据，以及更正格式、缺失值和拼写错误。

### 为什么数据清洗非常重要？
当一家公司使用数据来推动决策制定时，使用相关、完整和准确的数据至关重要。但是，数据集通常包含一些错误，必须在分析之前将它们删除。可能包括格式错误，例如错误的日期和货币，以及其他可能对预测产生重大影响的计量单位。异常值是一个特别值得关注的问题，因为它们总是会使结果发生偏差。其他常见的数据错误包括数据点损坏、信息缺失和印刷错误。干净的数据有助于生成高度准确的 ML 模型。 

干净、准确的数据对于训练 ML 模型尤其重要，因为使用糟糕的训练数据集可能会导致部署模型做出错误预测。这是数据科学家花费大量时间为 ML 准备数据的主要原因。

### 如何验证您的数据是否“干净”？
数据清洗过程包含几个必需的步骤，旨在识别和修复问题条目。第一步是分析数据以识别错误。这可能涉及通过使用规则、模式和约束来识别无效值的定性分析工具。下一步是删除或更正错误。 

常见的数据清洗步骤包括修复：
重复数据：删除重复信息
不相关数据：确定特定分析的关键字段并从分析中删除不相关数据
异常值：异常值会显著影响模型性能，因此请识别异常值并确定适当的操作
缺失数据：标记并删除或估算缺失数据
结构错误：纠正印刷错误和其他不一致，并使数据符合通用模式或约定


### 如何实现数据清洗？
#### 步骤1：数据清洗和噪声减少
我们首先移除不提供意义的符号或字符，例如HTML标签（在抓取的情况下）、XML解析、JSON、表情符号和哈希标签。不必要的字符经常会混淆模型，并增加上下文标记的数量，因此增加计算成本。

认识到没有放之四海而皆准的解决方案，我们将使用常见的清洗技术，根据不同的问题和文本类型调整我们的方法：

· 分词：将文本分割成单独的单词或标记。
· 去除噪声：消除不需要的符号、表情符号、哈希标签和Unicode字符。
· 标准化：将文本转换为小写，以保持一致性。
· 去除停用词：丢弃不增加意义的常见或重复单词，如“a”、“in”、“of”和“the”。
· 词形还原或词干提取：将单词还原到基本形式或词根形式。

以这条推文为例：
“I love coding! ? #PythonProgramming is fun! ?✨ Let’s clean some text ?”

尽管这对人类来说意思很明确，但对于LLM来说，需要更加清晰和简化。让我们通过应用Python中的常见技术来简化它，以便模型理解

这个过程去除了不相关的字符，留下了我们的模型可以理解的干净而有意义的文本：[‘love’, ‘coding’, ‘pythonprogramming’, ‘fun’, ‘clean’, ‘text’]()。

不知道读者有没有注意，原先推文中的主语I也被移除了？
因为在英语中，“I”被视为停用词（stop word）。停用词通常是指那些在文本中频繁出现但又没有太多实际含义的词汇，如“the”、“is”、“in”等。在很多NLP应用中，移除这些词有助于减少数据维度和计算负担，让模型更关注于那些有实际含义，能够带来区分度的词汇。

然而，是否移除某个词汇（包括“I”）取决于您的具体应用场景和需求。如果在您的应用中“我（I）”是有意义的，您希望保留它，那么您可以在去除停用词之前，从停用词列表中将它移除。这可以通过简单修改代码来实现。

#### 步骤2：文本标准化和规范化
接下来，我们应该始终优先考虑整个文本的一致性和连贯性。这对于确保准确的检索和生成至关重要。在下面的Python示例中，让我们扫描我们的文本输入，寻找拼写错误和其他可能导致准确性降低和性能下降的不一致性。
![accesswximg][image-1]
上面代码只是一个简单的验证，它对于大规模文本或更复杂的拼写错误纠正，仅依赖预定义的错误及其纠正可能不够。在这种情况下，可能需要考虑使用更先进的拼写检查库，例如pyspellchecker或使用基于机器学习的方法。通过连贯、一致的文本表示，我们的模型现在可以生成准确和上下文相关的响应。这个过程还使语义搜索能够提取最优的上下文块，特别是在RAG的背景下。

#### 步骤3：元数据处理
元数据收集，如识别重要的关键词和实体，使我们容易识别文本中可以用来改善语义搜索结果的元素，尤其是在企业应用中，如内容推荐系统。这一过程为模型提供额外的上下文，通常是提高RAG性能所必需的。

看一段代码，代码的作用是：

准备工作：首先，代码用了一个叫spaCy的工具，这是一个很强大的自然语言处理（NLP）库。它可以帮我们理解和分析人类的语言。为了让spaCy能理解英语，我们加载了一个叫en\_core\_web\_sm的英语模型。这就像给spaCy安装了一个英语词典，让它能认识和理解英语单词和句子的意思。

处理文本：接着，我们给spaCy一个任务——读一段英文文本。这段文本是关于未来科技趋势的博客文章片段，提到了人工智能、机器学习、谷歌、微软等内容。

找人名、地名、公司名：spaCy的任务是从这段文本中找出重要的名词，比如人名（John Doe）、公司名（Google, Microsoft）等。这些特定的名词被称为“命名实体”，它们在文本中通常代表一些重要的信息。

提取信息：找到这些命名实体后，spaCy会告诉我们每一个实体的具体文本（比如“Google”）和它是什么类型的实体（比如“公司名”）。这些信息被整理成一份列表，每个元素都是一个包含实体文本和类型标签的小字典。

输出成易读格式：最后，为了让这些提取出来的信息更方便查看和使用，代码把这份列表转换成了JSON格式的字符串。JSON是一种存储和传输数据的格式，看起来就像是一连串的键值对，既易于人阅读，也方便计算机处理。

总的来说，这段代码就是让spaCy读一段英文文本，然后找出里面的重要名词（如人名、公司名），并把这些信息整理成一种易于阅读和使用的格式。
![accesswximg1][image-2]

输出：
	[
	    {
	        "text": "'The Top 10 Tech Trends",
	        "label": "WORK_OF_ART"
	    },
	    {
	        "text": "2024",
	        "label": "DATE"
	    },
	    {
	        "text": "John",
	        "label": "PERSON"
	    },
	    {
	        "text": "Google",
	        "label": "ORG"
	    },
	    {
	        "text": "Microsoft",
	        "label": "ORG"
	    },
	    {
	        "text": "AI",
	        "label": "ORG"
	    }
	]
代码突出显示了spaCy的实体识别能力，识别文本中的日期、人物和组织等重要实体。这有助于RAG应用更好地理解上下文和词语之间的关系。

#### 步骤4：上下文信息处理
当使用LLMs时，您可能通常会处理多种语言或管理充满各种主题的庞大文档，这对您的模型来说可能难以理解。让我们看看两种可以帮助您的模型更好地理解数据的技术。

我们首先使用Translation API，将原始文本“Hello, how are you?”从英语翻译成西班牙语（翻译成中文纯为验证API）。
![accesswximg2][image-3]
用主题建模包括像数据聚类这样的技术识别文档的主题，并快速浏览大量信息。
下面代码使用了Latent Dirichlet Allocation（LDA）来进行主题建模。它的目的是从一组文本文档中自动发现隐含的主题。通过分析文档中单词的共现模式，LDA可以帮助我们识别出哪些单词经常一起出现，从而推断出文档集合中的主题。这种方法主要用于探索性数据分析，了解大型文本集合的主要主题。
![accesswximg3][image-4]

#### 二、验证基本数据清晰对GPT的反应
接下来，我们只验证基础清洗，即上一小节第一步清洗后，GPT基于数据做RAG回复的差别。
	synthetic_text = """
	Sarah (S): Technology Enthusiast
	Mark (M): AI Expert
	S: Hey Mark! How's it going? Heard about the latest advancements in Generative AI (GA)?
	M: Hey Sarah! Yes, I've been diving deep into the realm of GA lately. It's fascinating how it's shaping the future of technology!
	S: Absolutely! I mean, GA has been making waves across various industries. What do you think is driving its significance?
	M: Well, GA, especially Retrieval Augmented Generative (RAG), is revolutionizing content generation. It's not just about regurgitating information anymore; it's about creating contextually relevant and engaging content.
	S: Right! And with Machine Learning (ML) becoming more sophisticated, the possibilities seem endless.
	M: Exactly! With advancements in ML algorithms like GPT (Generative Pre-trained Transformer), we're seeing unprecedented levels of creativity in AI-generated content.
	S: But what about concerns regarding bias and ethics in GA?
	M: Ah, the age-old question! While it's true that GA can inadvertently perpetuate biases present in the training data, there are techniques like Adversarial Training (AT) that aim to mitigate such issues.
	S: Interesting! So, where do you see GA headed in the next few years?
	M: Well, I believe we'll witness a surge in applications leveraging GA for personalized experiences. From virtual assistants to content creation tools, GA will become ubiquitous in our daily lives.
	S: That's exciting! Imagine AI-powered virtual companions tailored to our preferences.
	M: Indeed! And with advancements in Natural Language Processing (NLP) and computer vision, these virtual companions will be more intuitive and lifelike than ever before.
	S: I can't wait to see what the future holds!
	M: Agreed! It's an exciting time to be in the field of AI.
	S: Absolutely! Thanks for sharing your insights, Mark.
	M: Anytime, Sarah. Let's keep pushing the boundaries of Generative AI together!
	S: Definitely! Catch you later, Mark!
	M: Take care, Sarah!
	"""

#### 基本清理
从对话中移除表情符号、哈希标签和Unicode字符。
![accesswximg4][image-5]
输出：
![accesswximg5][image-6]

然后，针对GPT4-0125，使用如下prompt：
	MESSAGE_SYSTEM_CONTENT = """You are a customer service agent that helps   
	a customer with answering questions. Please answer the question based on the  
	provided context below.   
	Make sure not to make any changes to the context if possible,  
	when prepare answers so as to provide accurate responses. If the answer   
	cannot be found in context, just politely say that you do not know,   
	do not try to make up an answer."""
问题：
	
	 "What are some specific techniques in Adversarial Training (AT) 
	that can help mitigate biases in Generative AI models?"

针对这个问题，分别使用基本清洗之前和之后的数据进行询问，GPT4-0125的回答如下。

清洗前数据的回答：
I'm sorry, but the provided conversation between Sarah and Mark does not include specific techniques in Adversarial Training (AT) that can help mitigate biases in Generative AI models. They mention the importance and the potential of Adversarial Training in addressing bias, but they do not delve into specific techniques or methodologies. If you have any other questions or need further clarification on a different topic, feel free to ask!

清洗后数据的回答：
I'm sorry, but the provided context does not specifically mention techniques in Adversarial Training (AT) that can help mitigate biases in Generative AI models. Adversarial Training is indeed a method used to improve the robustness of AI models, including generative ones, by exposing them to adversarial examples during training. However, the conversation primarily focuses on the general advancements and potentials of Generative AI, without delving into specific techniques for bias mitigation. If you have any other questions or need further information on a different topic, feel free to ask!

翻译成中文对照看，数据清晰的效果还是很明显的：
![accesswximg6][image-7]
从清洗前到清洗后的文本，我们可以看到几个关键的变化点，这些变化体现了对原始信息的提炼、精简和重组：

精简与重组：
清洗后的文本更为直接地指出了所提供上下文的局限性，即未具体提及对抗训练中减轻偏差的技术。同时，通过对比，清洗后的文本提供了对抗训练的简要解释，这有助于读者理解上下文的缺失部分。

详细程度：
清洗后的文本增加了对对抗训练作用的描述，即“提高人工智能模型鲁棒性的方法，具体做法是在训练过程中将模型暴露在对抗示例中。”这种额外的信息提供了一种更全面的视角，帮助读者更好地理解对抗训练的概念和目的。

焦点明确：
在清洗后的文本中，更清晰地强调了对话主要集中在生成式人工智能的一般进步和潜力上，并没有深入到减轻偏差的具体技术。这种明确的焦点有助于引导读者对信息的理解，避免对可用信息的误解。

互动邀请：
两种版本都以一种鼓励提问的方式结束，但清洗后的文本通过稍微不同的措辞强化了这一邀请，使其看起来更为友好和开放。

总体来说，从清洗前到清洗后的文本变化，展现了信息的压缩、重点的明确化以及对读者更友好的表达方式。这些变化有助于提高文本的可读性和信息的准确传达，同时保持了对进一步交流的开放态度。





[image-1]:	/images/accesswximg.png
[image-2]:	/images/accesswximg1.png
[image-3]:	/images/accesswximg2.png
[image-4]:	/images/accesswximg3.png
[image-5]:	/images/accesswximg4.png
[image-6]:	/images/accesswximg5.png
[image-7]:	/images/accesswximg6.png